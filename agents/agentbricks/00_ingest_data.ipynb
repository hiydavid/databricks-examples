{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Parquet Files to Delta Tables\n",
    "\n",
    "This notebook ingests the three parquet files as Spark DataFrames and writes them to Unity Catalog as Delta tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nCATALOG_NAME = \"your_catalog_name\"  # Replace with your catalog name\nSCHEMA_NAME = \"your_schema_name\"    # Replace with your schema name\n\n# Local data directory (relative to notebook location)\nLOCAL_DATA_PATH = \"./data\"\n\n# Target schema for Delta tables\nTARGET_SCHEMA = f\"{CATALOG_NAME}.{SCHEMA_NAME}\"\n\nprint(f\"Reading from: {LOCAL_DATA_PATH}\")\nprint(f\"Writing to schema: {TARGET_SCHEMA}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Schema if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schema if it doesn't exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {TARGET_SCHEMA}\")\n",
    "print(f\"Schema {TARGET_SCHEMA} created or already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ingest Balance Sheet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Read balance sheet parquet file\nbalance_sheet_path = f\"{LOCAL_DATA_PATH}/balance-sheet.parquet\"\nbalance_sheet_df = spark.read.parquet(balance_sheet_path)\n\nprint(\"Balance Sheet Data:\")\nprint(f\"Rows: {balance_sheet_df.count()}\")\nprint(f\"Columns: {len(balance_sheet_df.columns)}\")\nprint(\"\\nSchema:\")\nbalance_sheet_df.printSchema()\nprint(\"\\nSample data:\")\nbalance_sheet_df.show(5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write balance sheet to Delta table\n",
    "balance_sheet_table = f\"{TARGET_SCHEMA}.balance_sheet\"\n",
    "\n",
    "balance_sheet_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(balance_sheet_table)\n",
    "\n",
    "print(f\"✅ Balance sheet data written to {balance_sheet_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ingest Income Statement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Read income statement parquet file\nincome_statement_path = f\"{LOCAL_DATA_PATH}/income-statement.parquet\"\nincome_statement_df = spark.read.parquet(income_statement_path)\n\nprint(\"Income Statement Data:\")\nprint(f\"Rows: {income_statement_df.count()}\")\nprint(f\"Columns: {len(income_statement_df.columns)}\")\nprint(\"\\nSchema:\")\nincome_statement_df.printSchema()\nprint(\"\\nSample data:\")\nincome_statement_df.show(5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write income statement to Delta table\n",
    "income_statement_table = f\"{TARGET_SCHEMA}.income_statement\"\n",
    "\n",
    "income_statement_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(income_statement_table)\n",
    "\n",
    "print(f\"✅ Income statement data written to {income_statement_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ingest SEC 10K Chunked Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Read SEC 10K chunked parquet file\nsec_10k_path = f\"{LOCAL_DATA_PATH}/sec-10k-chunked.parquet\"\nsec_10k_df = spark.read.parquet(sec_10k_path)\n\nprint(\"SEC 10K Chunked Data:\")\nprint(f\"Rows: {sec_10k_df.count()}\")\nprint(f\"Columns: {len(sec_10k_df.columns)}\")\nprint(\"\\nSchema:\")\nsec_10k_df.printSchema()\nprint(\"\\nSample data:\")\nsec_10k_df.show(5, truncate=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write SEC 10K chunked data to Delta table\n",
    "sec_10k_table = f\"{TARGET_SCHEMA}.sec_10k_chunked\"\n",
    "\n",
    "sec_10k_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(sec_10k_table)\n",
    "\n",
    "print(f\"✅ SEC 10K chunked data written to {sec_10k_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Delta Tables Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in the schema\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {TARGET_SCHEMA}\")\n",
    "print(f\"Tables in {TARGET_SCHEMA}:\")\n",
    "tables_df.show()\n",
    "\n",
    "# Get table details\n",
    "table_names = [\n",
    "    f\"{TARGET_SCHEMA}.balance_sheet\",\n",
    "    f\"{TARGET_SCHEMA}.income_statement\", \n",
    "    f\"{TARGET_SCHEMA}.sec_10k_chunked\"\n",
    "]\n",
    "\n",
    "print(\"\\nTable Details:\")\n",
    "for table in table_names:\n",
    "    try:\n",
    "        count = spark.table(table).count()\n",
    "        print(f\"  {table}: {count:,} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {table}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Queries on Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query on balance sheet\n",
    "print(\"Sample Balance Sheet Query:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {TARGET_SCHEMA}.balance_sheet \n",
    "    LIMIT 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query on income statement\n",
    "print(\"Sample Income Statement Query:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {TARGET_SCHEMA}.income_statement \n",
    "    LIMIT 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query on SEC 10K data\n",
    "print(\"Sample SEC 10K Query:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * \n",
    "    FROM {TARGET_SCHEMA}.sec_10k_chunked \n",
    "    LIMIT 3\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ All three parquet files have been successfully ingested as Delta tables:\n",
    "\n",
    "1. **Balance Sheet**: `{catalog}.{schema}.balance_sheet`\n",
    "2. **Income Statement**: `{catalog}.{schema}.income_statement` \n",
    "3. **SEC 10K Chunked**: `{catalog}.{schema}.sec_10k_chunked`\n",
    "\n",
    "These tables are now ready to be used for:\n",
    "- Creating a Genie Space (balance_sheet and income_statement tables)\n",
    "- Creating a Vector Search Index (sec_10k_chunked table)\n",
    "- Building Agent Bricks components"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}