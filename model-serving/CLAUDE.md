# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

This is a Databricks model serving examples repository focused on serving AI/ML models, specifically the Flux.1 Dev image-to-image generation model. The repository demonstrates how to deploy and serve models using Databricks Model Serving infrastructure.

## Architecture

### Core Components

- **Model Wrapper Classes**: Python classes that implement `mlflow.pyfunc.PythonModel` to wrap external models for serving
- **Deployment Notebooks**: Jupyter notebooks that handle model registration, logging, and deployment to Databricks Model Serving
- **Unity Catalog Integration**: Models are registered in Unity Catalog with format `{catalog}.{schema}.{model_name}`

### Key Patterns

- **MLflow Integration**: All models use MLflow for logging, registration, and serving
- **GPU Memory Management**: Memory optimization techniques including model sharding, quantization, and explicit memory management
- **Volume-based Artifacts**: Large model files are stored in Databricks Unity Catalog Volumes and referenced as MLflow artifacts
- **Base64 Image Handling**: Images are converted to/from base64 for API compatibility

## Development Workflow

### Model Development

1. Create model wrapper class inheriting from `mlflow.pyfunc.PythonModel`
2. Implement `load_context()` for model initialization
3. Implement `predict()` for inference
4. Define MLflow signature with input/output schemas
5. Log and register model with MLflow

### Deployment Process

1. Use Databricks notebooks to register models
2. Configure Model Serving endpoints with appropriate GPU resources
3. Set environment variables (e.g., `FLUX_MODEL_PATH`) in serving configuration
4. Test deployed endpoints using MLflow deployment client

## Project Structure

```
flux1-dev/
├── Deploy-Flux1Dev.ipynb          # Main deployment notebook (HuggingFace download)
├── Deploy-Flux1Dev-Local.ipynb    # Local deployment notebook (Volume-based)
├── flux1dev_model.py              # Model wrapper (generated by notebooks)
├── requirements.txt               # Python dependencies
└── README.md                      # Project documentation
```

## Key Configuration

### Model Serving Requirements

- GPU Medium (A10G x 8) for Flux.1 models
- Small 0-4 Concurrency setting
- Scale to zero enabled for development
- 20-30 minute deployment time

### Required Environment Variables

- `FLUX_MODEL_PATH`: Path to model artifacts in Unity Catalog Volume
- `HUGGING_FACE_HUB_TOKEN`: For downloading models from HuggingFace (stored in Databricks secrets)

### Standard Dependencies

All models require these core packages:

- `mlflow-skinny[databricks]` for model serving
- `transformers`, `torch`, `diffusers` for AI model support
- `accelerate`, `bitsandbytes` for GPU optimization

## Common Tasks

### Running Notebooks

Execute notebooks in Databricks workspace with Serverless GPU Compute for model registration.

### Testing Deployments

Use `mlflow.deployments.get_deploy_client("databricks")` to test served models programmatically.

### Model Updates

Re-run deployment notebooks to create new model versions and update serving endpoints.

## Common Issues and Solutions

### Tensor Precision Mismatches

When encountering "Input type (float) and bias type (c10::Half) should be the same" errors:

- Ensure all model components (text_encoder, transformer) are loaded with consistent `torch_dtype=torch.float16`
- Pass PIL Images directly to pipelines instead of pre-converting to tensors
- Verify model artifacts were saved with consistent precision

### Model Loading Patterns

- **HuggingFace Direct**: Download models during serving initialization (Deploy-Flux1Dev.ipynb)
- **Volume-based**: Pre-download models to Unity Catalog Volumes for faster initialization (Deploy-Flux1Dev-Local.ipynb)
- Always specify `torch_dtype=torch.float16` for all model components when loading from local artifacts
