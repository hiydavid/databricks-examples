{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "badd4ad3-d618-4162-9d15-9eceb0e488cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Custom GLM Model Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e4e2139-3677-4928-85b3-c0e88f1224a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install polars --quiet\n",
    "%pip install mlflow --upgrade --pre --quiet\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d0e82e9-965d-4177-b9d9-eb26422af2bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"users\"  # TODO\n",
    "SCHEMA = \"your_schema\"  # TODO\n",
    "VOLUME = \"custom_glm\"  # TODO\n",
    "EXPERIMENT_NAME = \"glm-model-serving\"  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95b9a07e-17ea-494c-9529-e227bd34fa03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.exceptions import RestException\n",
    "\n",
    "experiment_name = f\"{os.getcwd()}/{EXPERIMENT_NAME}\"\n",
    "\n",
    "try:\n",
    "    mlflow.create_experiment(name=experiment_name)\n",
    "    print(f\"Creating new experiment {experiment_name}\")\n",
    "except RestException as e:\n",
    "    if \"RESOURCE_ALREADY_EXISTS\" in str(e):\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        print(f\"Experiment {experiment_name} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59164c33-3dc6-468b-8a58-dd8219b81850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Train a generic GLM model\n",
    "Here we train a gerenic GLM model and produce a JSON with the model parameters as output, and then use this as basis for a custom Python model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3acb7f32-b568-436f-9be2-9315117d7719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# load data\n",
    "data = sm.datasets.star98.load()\n",
    "data.exog = sm.add_constant(data.exog, prepend=False)\n",
    "\n",
    "# train model\n",
    "glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())\n",
    "res = glm_binom.fit()\n",
    "\n",
    "# get fitted params\n",
    "params = {\n",
    "    \"intercept\": float(res.params[\"const\"]),\n",
    "    \"coefficients\": {\n",
    "        name: float(val) for name, val in res.params.items() if name != \"const\"\n",
    "    },\n",
    "}\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15e742e9-2040-4982-b29d-8c3441a5eb3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save model params as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd53fb6f-834b-448e-ac9c-40584b62e129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "params_json = json.dumps(params)\n",
    "output_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/glm_params.json\"\n",
    "dbutils.fs.put(output_path, params_json, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d7025b7-8e42-43b8-9a78-8b5e3e088b4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22ebca2-8c20-4b6b-a6b5-8afb46e47876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df = data.raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ea9c21d-6450-4b5d-9a69-a40da5da13a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Construct custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352851cd-366a-418f-b755-eaa8482395fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "params_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/glm_params.json\"\n",
    "params_json = dbutils.fs.head(params_path)\n",
    "model_params = json.loads(params_json)\n",
    "\n",
    "feature_cols = list(model_params[\"coefficients\"].keys())\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bc57193-663d-4d78-a0aa-3a6fc6796f20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class GLMCustomModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model_params):\n",
    "        self.model_params = model_params\n",
    "\n",
    "    def load_context(self, context):\n",
    "        self.intercept = self.model_params[\"intercept\"]\n",
    "        self.coefs = self.model_params[\"coefficients\"]\n",
    "        self.feature_order = list(self.coefs.keys())\n",
    "\n",
    "    def _score_logit(self, pl_df: pl.DataFrame) -> pl.Series:\n",
    "        expr = pl.lit(self.intercept)\n",
    "        for col, coef in self.coefs.items():\n",
    "            expr = expr + pl.col(col) * coef\n",
    "        return pl_df.select(expr.alias(\"logit\")).to_series()\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        pl_df = pl.from_pandas(model_input[self.feature_order], rechunk=False)\n",
    "        logit = self._score_logit(pl_df)\n",
    "        prob = 1.0 / (1.0 + np.exp(-logit))\n",
    "        out = prob.to_frame().to_pandas()\n",
    "        out.columns = [\"probability_NABOVE\"]\n",
    "        return out\n",
    "\n",
    "\n",
    "glm_model = GLMCustomModel(model_params=model_params)\n",
    "glm_model.load_context(context=None)\n",
    "\n",
    "preds = glm_model.predict(context=None, model_input=test_df)\n",
    "\n",
    "display(preds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff84b593-79fa-4bfb-9483-d76c7c430c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log and register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf4f423-7b83-4ba7-b228-631e238f6e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"glm_custom_model\") as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"glm_model\",\n",
    "        artifacts={\n",
    "            \"glm_params\": f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/glm_params.json\"\n",
    "        },\n",
    "        registered_model_name=f\"{CATALOG}.{SCHEMA}.custom_glm_model\",\n",
    "        python_model=GLMCustomModel(model_params=model_params),\n",
    "        pip_requirements=[\n",
    "            \"mlflow\",\n",
    "            \"polars==1.30.0\",\n",
    "            \"pandas\",\n",
    "            \"numpy\",\n",
    "        ],\n",
    "        input_example=test_df[feature_cols].head(3),\n",
    "        signature=mlflow.models.infer_signature(\n",
    "            test_df[feature_cols].head(3),\n",
    "            pd.DataFrame({\"probability_NABOVE\": [0.0, 0.0, 0.0]}),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "print(\"Model logged under run:\", run.info.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "625a1d52-37d1-4600-96b5-0572fd85acaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Get GLM params as artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbc09ab-bd60-4741-9f22-517efa1e0f1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "logged_models = mlflow.search_logged_models()\n",
    "\n",
    "filtered_models = logged_models[logged_models[\"source_run_id\"] == run.info.run_id]\n",
    "\n",
    "logged_model_id = filtered_models[\"model_id\"][0]\n",
    "\n",
    "# get from model artifact page\n",
    "artifact_uri = f\"dbfs:/databricks/mlflow-tracking/{run.info.experiment_id}/logged_models/{logged_model_id}/artifacts/\"\n",
    "mlflow.artifacts.list_artifacts(artifact_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066c7ce6-8727-4632-82bf-1a407f8146ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "logged_json_path = mlflow.artifacts.download_artifacts(\n",
    "    artifact_uri + \"/artifacts/glm_params.json\"\n",
    ")\n",
    "with open(logged_json_path, \"r\") as f:\n",
    "    logged_params = json.load(f)\n",
    "\n",
    "print(logged_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f65eba7-d0df-4304-ab20-3896b8dfccfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "logged_pickle_path = mlflow.artifacts.download_artifacts(\n",
    "    artifact_uri + \"/python_model.pkl\"\n",
    ")\n",
    "with open(logged_pickle_path, \"rb\") as f:\n",
    "    pickled_model = pickle.load(f)\n",
    "\n",
    "print(pickled_model.model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ddabfea-de3a-4c1a-848d-68b7796fb65b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "101065f7-b2df-4674-ae4b-19948c11ab4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedEntityInput\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "ENDPOINT_NAME = \"custom-glm-endpoint\"  # TODO\n",
    "\n",
    "served_entity = ServedEntityInput(\n",
    "    entity_name=f\"{CATALOG}.{SCHEMA}.custom_glm_model\",\n",
    "    entity_version=1,\n",
    "    workload_size=\"Small\",\n",
    "    scale_to_zero_enabled=True,\n",
    "    workload_type=\"CPU\",\n",
    ")\n",
    "\n",
    "w.serving_endpoints.create_and_wait(\n",
    "    name=ENDPOINT_NAME,\n",
    "    config=EndpointCoreConfigInput(served_entities=[served_entity]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b20947f3-83be-4b6e-bfb3-46c9bbf990f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd9fcad-58ae-4d14-a411-4489fcc227e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "DATABRICKS_HOST = (\n",
    "    \"https://your-workspace.cloud.databricks.com\"  # TODO: Your Databricks Host\n",
    ")\n",
    "DATABRICKS_TOKEN = dbutils.secrets.get(\"your_secret_scope\", \"your_secret_name\")\n",
    "\n",
    "\n",
    "def create_tf_serving_json(data):\n",
    "    return {\n",
    "        \"inputs\": (\n",
    "            {name: data[name].tolist() for name in data.keys()}\n",
    "            if isinstance(data, dict)\n",
    "            else data.tolist()\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "def score_model(dataset):\n",
    "    url = f\"{DATABRICKS_HOST}/serving-endpoints/{ENDPOINT_NAME}/invocations\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    ds_dict = (\n",
    "        {\"dataframe_split\": dataset.to_dict(orient=\"split\")}\n",
    "        if isinstance(dataset, pd.DataFrame)\n",
    "        else create_tf_serving_json(dataset)\n",
    "    )\n",
    "    data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "    response = requests.request(method=\"POST\", headers=headers, url=url, data=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            f\"Request failed with status {response.status_code}, {response.text}\"\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "preds = score_model(test_df.head())\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "custom-glm-model-serving",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
