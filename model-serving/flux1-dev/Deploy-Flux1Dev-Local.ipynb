{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32153d29-0c2d-4271-b560-fe80d404269a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Serving `FLUX.1-Dev` Model From UC Volume\n",
    "\n",
    "Use an intereactive GPU cluster with 16.4 LTS. Example:\n",
    "* g4dn.8xlarge [T4] cluster with 18 GB\n",
    "* This is just a model logging and registration so no need for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf49d36-c978-4245-ba5a-137f071bbfef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a6861a-d197-494d-94ed-7fe23b998049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "catalog_name = \"users\"  # TODO\n",
    "schema_name = \"david_huang\"  # TODO\n",
    "model_name = \"flux-auto-img2img-model\"  # TODO\n",
    "vol_name = \"flux1dev_artifact\"  # TODO\n",
    "model_file_name = \"flux1dev_model.py\"  # TODO\n",
    "volume_path = f\"/Volumes/{catalog_name}/{schema_name}/{vol_name}\"  # TODO\n",
    "\n",
    "print(volume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e75b5984-b705-4093-ba3f-5f9ff4ee6cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.exceptions import RestException\n",
    "\n",
    "experiment_name = f\"{os.getcwd()}/flux1dev-serving\"\n",
    "\n",
    "try:\n",
    "    mlflow.create_experiment(name=experiment_name)\n",
    "    print(f\"Creating new experiment {experiment_name}\")\n",
    "except RestException as e:\n",
    "    if \"RESOURCE_ALREADY_EXISTS\" in str(e):\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        print(f\"Experiment {experiment_name} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "622c64c5-0aeb-46ca-98f6-f3de83d51d42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download Model to UC Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e05f1fa-3f89-454d-bb46-49d8edb11bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import T5EncoderModel\n",
    "# from diffusers import FluxTransformer2DModel, FluxImg2ImgPipeline\n",
    "# import os\n",
    "\n",
    "# # Set your HF token for downloading\n",
    "# os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = dbutils.secrets.get(\"dhuang\", \"hf-api-key\")\n",
    "\n",
    "# # Download and save all components\n",
    "# pipeline = FluxImg2ImgPipeline.from_pretrained(\n",
    "#     \"black-forest-labs/FLUX.1-dev\",\n",
    "#     torch_dtype=torch.float16\n",
    "# )\n",
    "\n",
    "# # Save to Volume\n",
    "# pipeline.save_pretrained(volume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35116950-cc8a-413b-9126-69a0402d3a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a45341f-7be7-4748-b4b4-3c35c7f8893d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "\n",
    "class StableDiffusionImgToImg(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self):\n",
    "        self.pipe = None\n",
    "\n",
    "    def load_context(self, context):\n",
    "        import os\n",
    "\n",
    "        import torch\n",
    "        import transformers\n",
    "        from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n",
    "        from diffusers import FluxImg2ImgPipeline, FluxTransformer2DModel\n",
    "        from transformers import BitsAndBytesConfig as BitsAndBytesConfig\n",
    "        from transformers import T5EncoderModel\n",
    "\n",
    "        from pathlib import Path\n",
    "\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "        MAX_MEMORY = {i: \"24GB\" for i in range(torch.cuda.device_count())}\n",
    "\n",
    "        model_artifacts_path = context.artifacts[\"flux1_model_uc\"]\n",
    "\n",
    "        text_encoder_8bit = T5EncoderModel.from_pretrained(\n",
    "            Path(model_artifacts_path).joinpath(\"text_encoder_2\"),\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"balanced\",\n",
    "            max_memory=MAX_MEMORY,\n",
    "            local_files_only=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "\n",
    "        transformer_8bit = FluxTransformer2DModel.from_pretrained(\n",
    "            Path(model_artifacts_path).joinpath(\"transformer\"),\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"balanced\",\n",
    "            max_memory=MAX_MEMORY,\n",
    "            local_files_only=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "\n",
    "        self.flush()\n",
    "\n",
    "        self.pipeline = FluxImg2ImgPipeline.from_pretrained(\n",
    "            Path(model_artifacts_path),\n",
    "            text_encoder_2=text_encoder_8bit,\n",
    "            transformer=transformer_8bit,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"balanced\",\n",
    "            max_memory=MAX_MEMORY,\n",
    "            local_files_only=True,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "\n",
    "        self.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        import gc\n",
    "        import torch\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_max_memory_allocated()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    def image_to_base64(self, image):\n",
    "        from io import BytesIO\n",
    "        import base64\n",
    "\n",
    "        buffered = BytesIO()\n",
    "        image.save(buffered, format=\"JPEG\")\n",
    "        return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "    def base64_to_image(self, base64_string):\n",
    "        from io import BytesIO\n",
    "        import base64\n",
    "        import PIL\n",
    "        from PIL import Image\n",
    "\n",
    "        # Decode the base64 string\n",
    "        img_data = base64.b64decode(base64_string)\n",
    "\n",
    "        # Create a BytesIO object from the decoded data\n",
    "        buffer = BytesIO(img_data)\n",
    "\n",
    "        # Open the image using PIL\n",
    "        image = Image.open(buffer)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        import torchvision.transforms as T\n",
    "        import torch\n",
    "\n",
    "        entry_device = torch.device(\n",
    "            \"cuda:0\"\n",
    "        )  # Or use self.pipeline.device or check hf_device_map if unsure\n",
    "\n",
    "        prompt = model_input[\"prompt\"][0]\n",
    "        init_image = self.base64_to_image(model_input[\"init_image\"][0])\n",
    "\n",
    "        transform = T.Compose(\n",
    "            [\n",
    "                T.ToTensor(),  # (C,H,W), float in [0,1]\n",
    "                T.Lambda(lambda x: x.to(torch.float16)),  # dtype match\n",
    "            ]\n",
    "        )\n",
    "        init_image_tensor = transform(init_image).unsqueeze(\n",
    "            0\n",
    "        )  # Add batch dim if required\n",
    "        init_image_tensor = init_image_tensor.to(entry_device)\n",
    "\n",
    "        num_inference_steps = model_input[\"num_inference_steps\"][0]\n",
    "\n",
    "        strength = model_input[\"strength\"][0]\n",
    "\n",
    "        guidance_scale = model_input[\"guidance_scale\"][0]\n",
    "\n",
    "        image = self.pipeline(\n",
    "            prompt=prompt,\n",
    "            image=init_image_tensor,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            strength=strength,\n",
    "            guidance_scale=guidance_scale,\n",
    "        ).images[0]\n",
    "\n",
    "        return self.image_to_base64(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ff5446d-27b1-45b8-815c-5dcae7c3eecc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Log & Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cdcff9b-c8ef-4ee4-a437-7c550541b9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec, TensorSpec\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "test_image_path = f\"{os.getcwd()}/test_image_4.jpg\"\n",
    "\n",
    "\n",
    "def load_image_from_volume(volume_path):\n",
    "    import PIL\n",
    "    from PIL import Image\n",
    "\n",
    "    with Image.open(volume_path) as img:\n",
    "        return img.convert(\"RGB\")\n",
    "\n",
    "\n",
    "def image_to_base64(image):\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"prompt\"),\n",
    "        ColSpec(DataType.integer, \"num_inference_steps\"),\n",
    "        ColSpec(DataType.string, \"init_image\"),\n",
    "        ColSpec(DataType.float, \"strength\"),\n",
    "        ColSpec(DataType.float, \"guidance_scale\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_schema = Schema([ColSpec(DataType.string, \"image\")])\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "\n",
    "image = image_to_base64(load_image_from_volume(test_image_path))\n",
    "\n",
    "\n",
    "# Define input example\n",
    "input_example = pd.DataFrame(\n",
    "    {\n",
    "        \"prompt\": [\"a photo of cat dressed in Gandalf from Lord of the Ring\"],\n",
    "        \"num_inference_steps\": [8],\n",
    "        \"init_image\": [image],\n",
    "        \"strength\": [0.8],\n",
    "        \"guidance_scale\": [12.0],\n",
    "    }\n",
    ")\n",
    "\n",
    "input_example[\"num_inference_steps\"] = input_example[\"num_inference_steps\"].astype(\n",
    "    \"int32\"\n",
    ")\n",
    "input_example[\"strength\"] = input_example[\"strength\"].astype(\"float32\")\n",
    "input_example[\"guidance_scale\"] = input_example[\"guidance_scale\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d0dc782-e4b7-4755-893c-06203127307b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log the model with its details such as artifacts, pip requirements and input example\n",
    "with mlflow.start_run() as run:\n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"model\",\n",
    "        python_model=StableDiffusionImgToImg(),\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        artifacts={\"flux1_model_uc\": f\"{volume_path}\"},\n",
    "        registered_model_name=f\"{catalog_name}.{schema_name}.{model_name}\",\n",
    "        pip_requirements=[\n",
    "            \"transformers==4.48.0\",\n",
    "            \"torch==2.5.1\",\n",
    "            \"torchvision==0.20.1\",\n",
    "            \"accelerate\",\n",
    "            \"diffusers==0.32.2\",\n",
    "            \"huggingface_hub==0.27.1\",\n",
    "            \"invisible-watermark>=0.2.0\",\n",
    "            \"bitsandbytes==0.45.4\",\n",
    "            \"sentencepiece==0.2.0\",\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dadcdd7c-666e-4c48-bb75-087fc73b8e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Serve on Mondel Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea23062a-f027-4ad6-8d51-48ac47708bc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Steps to serve the model on Model Serving:\n",
    "1. Use the following config when deploying on Model Serving UI:\n",
    "    - GPU Medium (A10G x 8)\n",
    "    - Small 0-4 Concurrency\n",
    "    - Scale to zero\n",
    "    - **DO NOT set FLUX_MODEL_PATH** - artifacts are packaged in the model\n",
    "2. Point to the registered model\n",
    "3. (Optional) Enable scale to zero during development / testing\n",
    "4. Wait 20-30 minutes\n",
    "\n",
    "**Important**: Unity Catalog volumes are packaged as MLflow artifacts. The model loads from `context.artifacts[\"flux_model\"]` automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e0e9e81-04d0-4760-9831-3a46293aa393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fad5d8d-eeee-41e3-a9df-7933e519fc11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "import pandas as pd\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Set the model serving endpoint name\n",
    "endpoint_name = \"dhuang-flux1dev\"  # TODO: Replace with your actual endpoint name\n",
    "\n",
    "# Create deployment client\n",
    "client = mlflow.deployments.get_deploy_client(\"databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edeb8ebd-96ea-44f7-af1d-85161fec85a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare test input\n",
    "test_input = pd.DataFrame(\n",
    "    {\n",
    "        \"prompt\": [\"a photo of a cat dressed as Don Corleone in the Godfather\"],\n",
    "        \"num_inference_steps\": [20],\n",
    "        \"init_image\": [image],  # Using the same test image from registration\n",
    "        \"strength\": [0.8],\n",
    "        \"guidance_scale\": [12.0],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Make prediction\n",
    "response = client.predict(\n",
    "    endpoint=endpoint_name, inputs={\"inputs\": test_input.to_dict(orient=\"records\")}\n",
    ")\n",
    "\n",
    "predicted_image_base64 = response[\"predictions\"]\n",
    "\n",
    "image_data = base64.b64decode(predicted_image_base64)\n",
    "result_image = Image.open(BytesIO(image_data))\n",
    "display(result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7284e2f0-195f-4ad5-bfc8-73e9ecb18865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_image.save(\"result_image.jpg\", format=\"JPEG\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "-r /Workspace/Users/david.huang@databricks.com/client_demos/model-serving/flux1-dev/requirements.txt"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Deploy-Flux1Dev-Local",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
